
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>QSAR.loss &#8212; QSAR 0.0.2 documentation</title>
    <link rel="stylesheet" href="../../_static/haiku.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
  </head><body>
      <div class="header" role="banner"><h1 class="heading"><a href="../../index.html">
          <span>QSAR 0.0.2 documentation</span></a></h1>
        <h2 class="heading"><span>QSAR.loss</span></h2>
      </div>
      <div class="topnav" role="navigation" aria-label="top navigation">
      
        <p>
        <a class="uplink" href="../../index.html">Contents</a>
        </p>

      </div>
      <div class="content" role="main">
        
        
  <h1>Source code for QSAR.loss</h1><div class="highlight"><pre>
<span></span><span class="c1">#implemented by modifying tensorflow code</span>
<span class="c1">#https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/contrib/losses/python/metric_learning/metric_loss_ops.py</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>


<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">sparse_tensor</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">control_flow_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">logging_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">script_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">sparse_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.summary</span> <span class="kn">import</span> <span class="n">summary</span>


<div class="viewcode-block" id="pairwise_distance"><a class="viewcode-back" href="../../api.html#QSAR.loss.pairwise_distance">[docs]</a><span class="k">def</span> <span class="nf">pairwise_distance</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the pairwise distance matrix with numerical stability.</span>
<span class="sd">  output[i, j] = || feature[i, :] - feature[j, :] ||_2</span>
<span class="sd">  </span>
<span class="sd">  :param feature: 2-D Tensor of size [number of data, feature dimension].</span>
<span class="sd">  :type feature: 2-D tensor</span>
<span class="sd">  :param squared: whether or not to square the pairwise distances, defaults to False</span>
<span class="sd">  :type squared: bool, optional</span>
<span class="sd">  :return: pairwise_distances</span>
<span class="sd">  :rtype: 2-D Tensor of size [number of data, number of data]</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">pairwise_distances_squared</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
      <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">feature</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
      <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
          <span class="n">math_ops</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">feature</span><span class="p">)),</span>
          <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
          <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span> <span class="o">-</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span>
                                                  <span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">feature</span><span class="p">))</span>

  <span class="c1"># Deal with numerical inaccuracies. Set small negatives to zero.</span>
  <span class="n">pairwise_distances_squared</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">pairwise_distances_squared</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
  <span class="c1"># Get the mask where the zero distances are at.</span>
  <span class="n">error_mask</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">less_equal</span><span class="p">(</span><span class="n">pairwise_distances_squared</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>

  <span class="c1"># Optionally take the sqrt.</span>
  <span class="k">if</span> <span class="n">squared</span><span class="p">:</span>
    <span class="n">pairwise_distances</span> <span class="o">=</span> <span class="n">pairwise_distances_squared</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">pairwise_distances</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
        <span class="n">pairwise_distances_squared</span> <span class="o">+</span>
        <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">error_mask</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1e-16</span><span class="p">)</span>

  <span class="c1"># Undo conditionally adding 1e-16.</span>
  <span class="n">pairwise_distances</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span>
      <span class="n">pairwise_distances</span><span class="p">,</span>
      <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">error_mask</span><span class="p">),</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

  <span class="n">num_data</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">feature</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="c1"># Explicitly set diagonals to zero.</span>
  <span class="n">mask_offdiagonals</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">pairwise_distances</span><span class="p">)</span> <span class="o">-</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span>
      <span class="n">array_ops</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">num_data</span><span class="p">]))</span>
  <span class="n">pairwise_distances</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">pairwise_distances</span><span class="p">,</span> <span class="n">mask_offdiagonals</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">pairwise_distances</span></div>



<div class="viewcode-block" id="masked_maximum"><a class="viewcode-back" href="../../api.html#QSAR.loss.masked_maximum">[docs]</a><span class="k">def</span> <span class="nf">masked_maximum</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the axis wise maximum over chosen elements.</span>

<span class="sd">  :param data: input data, 2-D float `Tensor` of size [n, m].</span>
<span class="sd">  :type data: tensor</span>
<span class="sd">  :param mask: mask tensor, 2-D Boolean `Tensor` of size [n, m].</span>
<span class="sd">  :type mask: tensor</span>
<span class="sd">  :param dim: [The dimension over which to compute the maximum, defaults to 1</span>
<span class="sd">  :type dim: int, optional</span>
<span class="sd">  :return:     masked_maximums: N-D `Tensor`, the maximized dimension is of \</span>
<span class="sd">    size 1 after the operation.</span>
<span class="sd">  :rtype: tensor</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">axis_minimums</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">masked_maximums</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span>
      <span class="n">math_ops</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">data</span> <span class="o">-</span> <span class="n">axis_minimums</span><span class="p">,</span> <span class="n">mask</span><span class="p">),</span> <span class="n">dim</span><span class="p">,</span>
      <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">axis_minimums</span>
  <span class="k">return</span> <span class="n">masked_maximums</span></div>


<div class="viewcode-block" id="masked_minimum"><a class="viewcode-back" href="../../api.html#QSAR.loss.masked_minimum">[docs]</a><span class="k">def</span> <span class="nf">masked_minimum</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the axis wise minimum over chosen elements.</span>

<span class="sd">  :param data: input data, 2-D float `Tensor` of size [n, m].</span>
<span class="sd">  :type data: tensor</span>
<span class="sd">  :param mask: mask tensor, 2-D Boolean `Tensor` of size [n, m].</span>
<span class="sd">  :type mask: tensor</span>
<span class="sd">  :param dim: [The dimension over which to compute the minimum, defaults to 1</span>
<span class="sd">  :type dim: int, optional</span>
<span class="sd">  :return:     masked_maximums: N-D `Tensor`, the minimized dimension is of \</span>
<span class="sd">    size 1 after the operation.</span>
<span class="sd">  :rtype: tensor</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">axis_maximums</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">masked_minimums</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span>
      <span class="n">math_ops</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">data</span> <span class="o">-</span> <span class="n">axis_maximums</span><span class="p">,</span> <span class="n">mask</span><span class="p">),</span> <span class="n">dim</span><span class="p">,</span>
      <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">axis_maximums</span>
  <span class="k">return</span> <span class="n">masked_minimums</span></div>


<div class="viewcode-block" id="triplet_semihard_loss"><a class="viewcode-back" href="../../api.html#QSAR.loss.triplet_semihard_loss">[docs]</a><span class="k">def</span> <span class="nf">triplet_semihard_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the triplet loss with semi-hard negative mining.</span>
<span class="sd">  The loss encourages the positive distances (between a pair of embeddings with</span>
<span class="sd">  the same labels) to be smaller than the minimum negative distance among</span>
<span class="sd">  which are at least greater than the positive distance plus the margin constant</span>
<span class="sd">  (called semi-hard negative) in the mini-batch. If no such negative exists,</span>
<span class="sd">  uses the largest negative distance instead.</span>
<span class="sd">  See: https://arxiv.org/abs/1503.03832.</span>
<span class="sd">  </span>
<span class="sd">  :param labels: 1-D tf.int32 `Tensor` with shape [batch_size] of multiclass \</span>
<span class="sd">    integer labels.</span>
<span class="sd">  :type labels: 1-D tf.int32 tensor</span>
<span class="sd">  :param embeddings: 2-D float `Tensor` of embedding vectors. Embeddings should\</span>
<span class="sd">    be l2 normalized.</span>
<span class="sd">  :type embeddings: 2-D float tensor</span>
<span class="sd">  :param margin: margin term in the loss definition, defaults to 1.0</span>
<span class="sd">  :type margin: float, optional</span>
<span class="sd">  :return: triplet_loss</span>
<span class="sd">  :rtype: tf.float32 scalar</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">unknown</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="c1"># Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.</span>
  <span class="n">lshape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
  <span class="c1">#assert lshape.shape == 1</span>
  <span class="n">labels</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">[</span><span class="n">lshape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">])</span>

  <span class="c1"># Build pairwise squared distance matrix.</span>
  <span class="n">pdist_matrix</span> <span class="o">=</span> <span class="n">pairwise_distance</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="c1"># Build pairwise binary adjacency matrix.</span>
  <span class="n">adjacency</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
  <span class="c1"># Invert so we can select negatives only.</span>
  <span class="n">adjacency_not</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">adjacency</span><span class="p">)</span>

  <span class="n">batch_size</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

  <span class="c1"># Compute the mask.</span>
  <span class="n">pdist_matrix_tile</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">pdist_matrix</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span>
      <span class="n">array_ops</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">adjacency_not</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
      <span class="n">math_ops</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span>
          <span class="n">pdist_matrix_tile</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
              <span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">pdist_matrix</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])))</span>
  <span class="n">mask_final</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
      <span class="n">math_ops</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span>
          <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
              <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
          <span class="mf">0.0</span><span class="p">),</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">])</span>
  <span class="n">mask_final</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">mask_final</span><span class="p">)</span>

  <span class="n">adjacency_not</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">adjacency_not</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

  <span class="c1"># negatives_outside: smallest D_an where D_an &gt; D_ap.</span>
  <span class="n">negatives_outside</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
      <span class="n">masked_minimum</span><span class="p">(</span><span class="n">pdist_matrix_tile</span><span class="p">,</span> <span class="n">mask</span><span class="p">),</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">])</span>
  <span class="n">negatives_outside</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">negatives_outside</span><span class="p">)</span>

  <span class="c1"># negatives_inside: largest D_an.</span>
  <span class="n">negatives_inside</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span>
      <span class="n">masked_maximum</span><span class="p">(</span><span class="n">pdist_matrix</span><span class="p">,</span> <span class="n">adjacency_not</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">])</span>
  <span class="n">semi_hard_negatives</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
      <span class="n">mask_final</span><span class="p">,</span> <span class="n">negatives_outside</span><span class="p">,</span> <span class="n">negatives_inside</span><span class="p">)</span>

  <span class="n">loss_mat</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="n">pdist_matrix</span> <span class="o">-</span> <span class="n">semi_hard_negatives</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">unknown</span><span class="p">:</span>
      <span class="n">bool_mask</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">mask_unknown</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">bool_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">loss_mat</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">mask_unknown</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">loss_mat</span><span class="p">,</span> <span class="n">mask_unknown</span><span class="p">))))</span>

  <span class="n">mask_positives</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
      <span class="n">adjacency</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">-</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span>
          <span class="n">array_ops</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span><span class="p">]))</span>

  <span class="c1"># In lifted-struct, the authors multiply 0.5 for upper triangular</span>
  <span class="c1">#   in semihard, they take all positive pairs except the diagonal.</span>
  <span class="n">num_positives</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">mask_positives</span><span class="p">)</span>

  <span class="n">triplet_loss</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">truediv</span><span class="p">(</span>
      <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
          <span class="n">math_ops</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
              <span class="n">math_ops</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">loss_mat</span><span class="p">,</span> <span class="n">mask_positives</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)),</span>
      <span class="n">num_positives</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="s1">&#39;triplet_semihard_loss&#39;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">triplet_loss</span></div>
</pre></div>

      </div>
      <div class="bottomnav" role="navigation" aria-label="bottom navigation">
      
        <p>
        <a class="uplink" href="../../index.html">Contents</a>
        </p>

      </div>

    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Xi Chen.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.0.
    </div>
  </body>
</html>